{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Lab 1\n",
    "### J. Martinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Install Gymnasium\n",
    "\n",
    "See the page [https://gymnasium.farama.org](Gymnasium).\n",
    "Check [https://github.com/Farama-Foundation/Gymnasium]() for installation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your installation of gymnasium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\praba\\anaconda3\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\praba\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\praba\\anaconda3\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "   ---------------------------------------- 0.0/958.1 kB ? eta -:--:--\n",
      "    -------------------------------------- 20.5/958.1 kB 640.0 kB/s eta 0:00:02\n",
      "   - ------------------------------------- 41.0/958.1 kB 487.6 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/958.1 kB 544.7 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/958.1 kB 544.7 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 71.7/958.1 kB 326.8 kB/s eta 0:00:03\n",
      "   ----- -------------------------------- 143.4/958.1 kB 566.5 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 153.6/958.1 kB 573.4 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 194.6/958.1 kB 588.9 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 204.8/958.1 kB 565.6 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 235.5/958.1 kB 576.2 kB/s eta 0:00:02\n",
      "   ---------- --------------------------- 276.5/958.1 kB 586.4 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 286.7/958.1 kB 570.1 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 337.9/958.1 kB 582.0 kB/s eta 0:00:02\n",
      "   -------------- ----------------------- 368.6/958.1 kB 587.5 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 399.4/958.1 kB 592.3 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 419.8/958.1 kB 595.3 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 450.6/958.1 kB 587.1 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 481.3/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 501.8/958.1 kB 592.9 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 522.2/958.1 kB 595.3 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 553.0/958.1 kB 598.4 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 563.2/958.1 kB 589.5 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 604.2/958.1 kB 593.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 614.4/958.1 kB 594.6 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 645.1/958.1 kB 588.4 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 686.1/958.1 kB 600.7 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 696.3/958.1 kB 593.0 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 727.0/958.1 kB 595.3 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 747.5/958.1 kB 597.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 778.2/958.1 kB 599.1 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 798.7/958.1 kB 593.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 829.4/958.1 kB 595.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 860.2/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 880.6/958.1 kB 592.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 911.4/958.1 kB 594.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 931.8/958.1 kB 589.6 kB/s eta 0:00:01\n",
      "   -------------------------------------  952.3/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  952.3/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  952.3/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  952.3/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  952.3/958.1 kB 591.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 958.1/958.1 kB 518.2 kB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import gymnasium as gym\n",
    "except:\n",
    "    !pip3 install gymnasium\n",
    "    import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is time, start implementing TicTacToe with 1/2 player(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def step(self, action, player):\n",
    "        if self.board[action] != 0:\n",
    "            raise ValueError(\"Invalid action!\")\n",
    "        self.board[action] = player\n",
    "\n",
    "        # Check for a winner\n",
    "        for i in range(3):\n",
    "            if all(self.board[i, :] == player) or all(self.board[:, i] == player):\n",
    "                self.done = True\n",
    "                self.winner = player\n",
    "                return self.board, 1 if player == 1 else -1, self.done\n",
    "\n",
    "        if all(self.board.diagonal() == player) or all(np.fliplr(self.board).diagonal() == player):\n",
    "            self.done = True\n",
    "            self.winner = player\n",
    "            return self.board, 1 if player == 1 else -1, self.done\n",
    "\n",
    "        if len(self.available_actions()) == 0:  # Draw\n",
    "            self.done = True\n",
    "            return self.board, 0, self.done\n",
    "\n",
    "        return self.board, 0, self.done\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, player, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.q_table = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.player = player\n",
    "\n",
    "    def get_state_key(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        state_key = self.get_state_key(state)\n",
    "        if np.random.rand() < self.epsilon or state_key not in self.q_table:\n",
    "            return random.choice(available_actions)\n",
    "\n",
    "        q_values = self.q_table[state_key]\n",
    "        max_q = max(q_values.values())\n",
    "        best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        state_key = self.get_state_key(state)\n",
    "        next_state_key = self.get_state_key(next_state)\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = {action: 0 for action in [(i, j) for i in range(3) for j in range(3)]}\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = {action: 0 for action in [(i, j) for i in range(3) for j in range(3)]}\n",
    "\n",
    "        current_q = self.q_table[state_key][action]\n",
    "        max_next_q = max(self.q_table[next_state_key].values())\n",
    "\n",
    "        self.q_table[state_key][action] = current_q + self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next_q - current_q\n",
    "        )\n",
    "\n",
    "# Training the agents\n",
    "game = TicTacToe()\n",
    "agent1 = QLearningAgent(player=1)\n",
    "agent2 = QLearningAgent(player=-1)\n",
    "\n",
    "for episode in range(10000):\n",
    "    state = game.reset()\n",
    "    while not game.done:\n",
    "        # Player 1's turn\n",
    "        action = agent1.choose_action(state, game.available_actions())\n",
    "        next_state, reward, done = game.step(action, player=1)\n",
    "        agent1.update_q_value(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            agent2.update_q_value(state, action, -reward, next_state)\n",
    "            break\n",
    "\n",
    "        # Player 2's turn\n",
    "        action = agent2.choose_action(state, game.available_actions())\n",
    "        next_state, reward, done = game.step(action, player=-1)\n",
    "        agent2.update_q_value(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            agent1.update_q_value(state, action, -reward, next_state)\n",
    "            break\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and implement a simple time difference update of p(win).\n",
    "Remember that the general definition of the TD update rule is:\n",
    "\n",
    "$$ V(s_t) \\leftarrow V(s_t) + \\alpha[ V(s_{t+1}) - V(s_{t}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]\n",
    "\n",
    "    def step(self, action, player):\n",
    "        if self.board[action] != 0:\n",
    "            raise ValueError(\"Invalid action!\")\n",
    "        self.board[action] = player\n",
    "\n",
    "        # Check for a winner\n",
    "        for i in range(3):\n",
    "            if all(self.board[i, :] == player) or all(self.board[:, i] == player):\n",
    "                self.done = True\n",
    "                self.winner = player\n",
    "                return self.board, 1 if player == 1 else -1, self.done\n",
    "\n",
    "        if all(self.board.diagonal() == player) or all(np.fliplr(self.board).diagonal() == player):\n",
    "            self.done = True\n",
    "            self.winner = player\n",
    "            return self.board, 1 if player == 1 else -1, self.done\n",
    "\n",
    "        if len(self.available_actions()) == 0:  # Draw\n",
    "            self.done = True\n",
    "            return self.board, 0, self.done\n",
    "\n",
    "        return self.board, 0, self.done\n",
    "\n",
    "class TDAgent:\n",
    "    def __init__(self, player, learning_rate=0.1):\n",
    "        self.values = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.player = player\n",
    "\n",
    "    def get_state_key(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "    def get_value(self, state):\n",
    "        state_key = self.get_state_key(state)\n",
    "        return self.values.get(state_key, 0.5)  # Default value of 0.5 for unknown states\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        return random.choice(available_actions)\n",
    "\n",
    "    def update_value(self, state, next_state):\n",
    "        state_key = self.get_state_key(state)\n",
    "        next_state_key = self.get_state_key(next_state)\n",
    "\n",
    "        current_value = self.values.get(state_key, 0.5)\n",
    "        next_value = self.values.get(next_state_key, 0.5)\n",
    "\n",
    "        self.values[state_key] = current_value + self.learning_rate * (next_value - current_value)\n",
    "\n",
    "# Training the agents\n",
    "game = TicTacToe()\n",
    "agent1 = TDAgent(player=1)\n",
    "agent2 = TDAgent(player=-1)\n",
    "\n",
    "for episode in range(10000):\n",
    "    state = game.reset()\n",
    "    while not game.done:\n",
    "        # Player 1's turn\n",
    "        action = agent1.choose_action(state, game.available_actions())\n",
    "        next_state, reward, done = game.step(action, player=1)\n",
    "        agent1.update_value(state, next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Player 2's turn\n",
    "        action = agent2.choose_action(state, game.available_actions())\n",
    "        next_state, reward, done = game.step(action, player=-1)\n",
    "        agent2.update_value(state, next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.11219 , -2.265905]), array([-2.153715]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries since the code environment has reset\n",
    "import numpy as np\n",
    "\n",
    "# Given data points\n",
    "t_values = np.array([1, 2, 4])\n",
    "y_values = np.array([-0.44796, -0.56015, -5.09196])\n",
    "\n",
    "# Compute first derivatives (finite differences)\n",
    "dy_dt = np.diff(y_values) / np.diff(t_values)\n",
    "\n",
    "# Compute second derivatives (finite differences)\n",
    "d2y_dt2 = np.diff(dy_dt) / np.diff(t_values[:-1])\n",
    "\n",
    "# Assign derivatives to their corresponding time points\n",
    "dy_dt_values = np.array([dy_dt[0], dy_dt[1]])  # dy/dt at t=1, t=2\n",
    "d2y_dt2_values = np.array([d2y_dt2[0]])  # d2y/dt2 at t=1\n",
    "\n",
    "# Display computed derivatives\n",
    "dy_dt_values, d2y_dt2_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random                                    \n",
    "board = np.array([['-', '-', '-'],\n",
    "                  ['-', '-', '-'],\n",
    "                  ['-', '-', '-']])             \n",
    "players = ['X', 'O']                             \n",
    "num_players = len(players)\n",
    "Q = {}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.5\n",
    "num_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  |  -  |  -\n",
      "---------------\n",
      "-  |  -  |  -\n",
      "---------------\n",
      "-  |  -  |  -\n",
      "---------------\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "def print_board(board):\n",
    "    for row in board:\n",
    "        print('  |  '.join(row))\n",
    "        print('---------------')\n",
    "print_board(board)\n",
    "\n",
    "\n",
    "# Function to convert the board state to a string to use it as a key in the Q-table dictionary.\n",
    "def board_to_string(board):\n",
    "    return ''.join(board.flatten())\n",
    "board_to_string(board)\n",
    "\n",
    "\n",
    "#defining action as a cell randomly selected from the empty cells\n",
    "empty_cells = np.argwhere(board == '-')\n",
    "action = tuple(random.choice(empty_cells))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the game is over by checking different winning condition\n",
    "\n",
    "def is_game_over(board):\n",
    "\n",
    "    # Check rows for winning condition\n",
    "    for row in board:\n",
    "        if len(set(row)) == 1 and row[0] != '-':        #len(set(row)) == 1 -> check if all elements in row are same and  none of the cell is empty\n",
    "            return True, row[0]\n",
    "\n",
    "\n",
    "    # Check columns\n",
    "    for col in board.T:                                 #iterate over clms of transponse of board\n",
    "        if len(set(col)) == 1 and col[0] != '-':\n",
    "            return True, col[0]\n",
    "\n",
    "\n",
    "    # Check diagonals\n",
    "    if len(set(board.diagonal())) == 1 and board[0, 0] != '-':             #check all elements in main diagonal are same and non empty\n",
    "        return True, board[0, 0]\n",
    "    if len(set(np.fliplr(board).diagonal())) == 1 and board[0, 2] != '-':   #horizontal flip the board and check...\n",
    "        return True, board[0, 2]\n",
    "\n",
    "\n",
    "    # Check if the board is full\n",
    "    if '-' not in board:\n",
    "        return True, 'draw'\n",
    "\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose an action based on the Q-table\n",
    "\n",
    "#Random exploration condition in the choose_action function checks whether agent should perform a random exploration or not or if current state is not present in the Q-table\n",
    "#if random exploration is choosen,\n",
    "#a random action is chosen from the available empty cells on the board.\n",
    "# This promotes exploration and allows the agent to try out different actions and gather more information about the environment.\n",
    "\n",
    "\n",
    "#if exploitation is choosen,\n",
    "#the function selects the action with the highest Q-value from the available empty cells.\n",
    "#and do action - > update it with player symbol (X or O according to player[])\n",
    "\n",
    "def choose_action(board, exploration_rate):\n",
    "    state = board_to_string(board)\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "    if random.uniform(0, 1) < exploration_rate or state not in Q:\n",
    "        # Choose a random action\n",
    "        empty_cells = np.argwhere(board == '-')\n",
    "        action = tuple(random.choice(empty_cells))\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        q_values = Q[state]\n",
    "        empty_cells = np.argwhere(board == '-')                                    #returns indices of the empty cells in the board.\n",
    "        empty_q_values = [q_values[cell[0], cell[1]] for cell in empty_cells]      #retrieves Q-values corresponding to each empty cells.\n",
    "        max_q_value = max(empty_q_values)                                          #find the maximum Q-value among the empty cells Qvalue\n",
    "        max_q_indices = [i for i in range(len(empty_cells)) if empty_q_values[i] == max_q_value]    #retrieves the indices of empty cells that have the maximum Q-value.\n",
    "        max_q_index = random.choice(max_q_indices)                                 #if there are multiple cells with same maximum Q value select 1 randomly\n",
    "        action = tuple(empty_cells[max_q_index])                                   #retrieves the indices of the selected empty cell based on max_q_index\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert the cell coordinates (row and column) of the chosen action to the next state of the board as a string.\n",
    "\n",
    "def board_next_state(cell):\n",
    "    next_state = board.copy()                      #create a copy of current board state\n",
    "    next_state[cell[0], cell[1]] = players[0]\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O  |  -  |  -\n",
      "---------------\n",
      "-  |  -  |  -\n",
      "---------------\n",
      "-  |  -  |  -\n",
      "---------------\n",
      "Enter the row (0-2): 0\n",
      "Enter the column (0-2): 2\n",
      "O  |  -  |  X\n",
      "---------------\n",
      "-  |  O  |  -\n",
      "---------------\n",
      "-  |  -  |  -\n",
      "---------------\n",
      "Enter the row (0-2): 2\n",
      "Enter the column (0-2): 2\n",
      "O  |  -  |  X\n",
      "---------------\n",
      "-  |  O  |  -\n",
      "---------------\n",
      "-  |  O  |  X\n",
      "---------------\n",
      "Enter the row (0-2): 0\n",
      "Enter the column (0-2): 1\n",
      "O  |  X  |  X\n",
      "---------------\n",
      "-  |  O  |  -\n",
      "---------------\n",
      "O  |  O  |  X\n",
      "---------------\n",
      "Enter the row (0-2): 1\n",
      "Enter the column (0-2): 2\n",
      "O  |  X  |  X\n",
      "---------------\n",
      "-  |  O  |  X\n",
      "---------------\n",
      "O  |  O  |  X\n",
      "---------------\n",
      "Human player wins!\n"
     ]
    }
   ],
   "source": [
    "# Function to update the Q-table\n",
    "agent_wins = 0\n",
    "# def update_q_table(state, action, next_state, reward):\n",
    "#     q_values = Q.get(state, np.zeros((3, 3)))                               #Retrieve the Q-values for a particular state from the Q-table dictionary Q.\n",
    "#     next_q_values = Q.get(board_to_string(next_state), np.zeros((3, 3)))       # Calculate the maximum Q-value for the next state from q table\n",
    "#     max_next_q_value = np.max(next_q_values)                                #find maxmium q values from q values of nxt state\n",
    "\n",
    "\n",
    "\n",
    "#     # Q-learning update equation\n",
    "#     q_values[action[0], action[1]] += learning_rate * (reward + discount_factor * max_next_q_value - q_values[action[0], action[1]])\n",
    "# #Q-learning update equation calculates the new Q-value for the current state-action pair based on the immediate reward, the discounted future rewards, and the current Q-value.\n",
    "# #By subtracting the current Q-value from the estimated total reward, it calculates the temporal difference (TD) error, which represents the discrepancy between the expected reward and the actual reward.\n",
    "\n",
    "\n",
    "# #The new Q-value is obtained by updating the current Q-value using the TD error, the learning rate, and the discount factor. This update process helps the Q-values to gradually converge towards the optimal values, reflecting the expected long-term rewards for each state-action pair.\n",
    "#     Q[state] = q_values\n",
    "\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    q_values = Q.get(state, np.zeros((3, 3)))\n",
    "\n",
    "    # Calculate the maximum Q-value for the next state\n",
    "    next_q_values = Q.get(board_to_string(next_state), np.zeros((3, 3)))\n",
    "    max_next_q_value = np.max(next_q_values)\n",
    "\n",
    "    # Q-learning update equation\n",
    "    q_values[action[0], action[1]] += learning_rate * (reward + discount_factor * max_next_q_value - q_values[action[0], action[1]])\n",
    "\n",
    "    Q[state] = q_values\n",
    "\n",
    "# Main Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    board = np.array([['-', '-', '-'],\n",
    "                      ['-', '-', '-'],\n",
    "                      ['-', '-', '-']])\n",
    "\n",
    "    current_player = random.choice(players)\n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        # Choose an action based on the current state\n",
    "        action = choose_action(board, exploration_rate)\n",
    "\n",
    "        # Make the chosen move\n",
    "        row, col = action\n",
    "        board[row, col] = current_player\n",
    "\n",
    "        # Check if the game is over\n",
    "        game_over, winner = is_game_over(board)\n",
    "\n",
    "        if game_over:\n",
    "            # Update the Q-table with the final reward\n",
    "            if winner == current_player:\n",
    "                reward = 1\n",
    "            elif winner == 'draw':\n",
    "                reward = 0.5\n",
    "            else:\n",
    "                reward = 0\n",
    "            update_q_table(board_to_string(board), action, board, reward)\n",
    "        else:\n",
    "            # Switch to the next player\n",
    "            current_player = players[(players.index(current_player) + 1) % num_players]\n",
    "\n",
    "        # Update the Q-table based on the immediate reward and the next state\n",
    "        if not game_over:\n",
    "            next_state = board_next_state(action)\n",
    "            update_q_table(board_to_string(board), action, next_state, 0)\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Play against the trained agent\n",
    "board = np.array([['-', '-', '-'],\n",
    "                  ['-', '-', '-'],\n",
    "                  ['-', '-', '-']])\n",
    "\n",
    "current_player = random.choice(players)\n",
    "game_over = False\n",
    "\n",
    "# ...\n",
    "\n",
    "while not game_over:\n",
    "    if current_player == 'X':\n",
    "        # Human player's turn\n",
    "        print_board(board)\n",
    "        row = int(input(\"Enter the row (0-2): \"))\n",
    "        col = int(input(\"Enter the column (0-2): \"))\n",
    "        action = (row, col)\n",
    "    else:\n",
    "        # Trained agent's turn\n",
    "        action = choose_action(board, exploration_rate=0)\n",
    "\n",
    "    row, col = action\n",
    "    board[row, col] = current_player\n",
    "\n",
    "    game_over, winner = is_game_over(board)\n",
    "\n",
    "    if game_over:\n",
    "        print_board(board)\n",
    "        if winner == 'X':\n",
    "            print(\"Human player wins!\")\n",
    "        elif winner == 'O':\n",
    "            print(\"Agent wins!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")\n",
    "    else:\n",
    "        current_player = players[(players.index(current_player) + 1) % num_players]\n",
    "\n",
    "#agent_win_percentage = (agent_wins / num_games) * 100\n",
    "#print(\"Agent win percentage: {:.2f}%\".format(agent_win_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main Q-learning algorithm\n",
    "# num_draws = 0  # Counter for the number of draws\n",
    "# agent_wins = 0  # Counter for the number of wins by the agent\n",
    "\n",
    "# for episode in range(num_episodes):\n",
    "#     board = np.array([['-', '-', '-'],\n",
    "#                       ['-', '-', '-'],\n",
    "#                       ['-', '-', '-']])\n",
    "\n",
    "#     current_player = random.choice(players)  # Randomly choose the current player\n",
    "#     game_over = False\n",
    "\n",
    "#     while not game_over:\n",
    "#         action = choose_action(board, exploration_rate)  # Choose an action using the exploration rate\n",
    "\n",
    "#         row, col = action\n",
    "#         board[row, col] = current_player  # Update the board with the current player's move\n",
    "\n",
    "#         game_over, winner = is_game_over(board)  # Check if the game is over and determine the winner\n",
    "\n",
    "#         if game_over:\n",
    "#             if winner == current_player:  # Agent wins\n",
    "#                 reward = 1\n",
    "#                 agent_wins += 1\n",
    "#             elif winner == 'draw':  # Game ends in a draw\n",
    "#                 reward = 0\n",
    "#                 num_draws += 1\n",
    "#             else:  # Agent loses\n",
    "#                 reward = -1\n",
    "#             update_q_table(board_to_string(board), action, board, reward)  # Update the Q-table\n",
    "#         else:\n",
    "#             current_player = players[(players.index(current_player) + 1) % num_players]  # Switch to the next player\n",
    "\n",
    "#         if not game_over:\n",
    "#             next_state = board_next_state(action)\n",
    "#             update_q_table(board_to_string(board), action, next_state, 0)  # Update the Q-table with the next state\n",
    "\n",
    "#     exploration_rate *= 0.99  # Decrease the exploration rate over time\n",
    "\n",
    "# # Play multiple games between the trained agent and itself\n",
    "# agent_win_percentage = (agent_wins / num_games) * 100\n",
    "# draw_percentage = (num_draws / num_games) * 100\n",
    "\n",
    "# print(\"Agent win percentage: {:.2f}%\".format(agent_win_percentage))\n",
    "# print(\"Draw percentage: {:.2f}%\".format(draw_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "AGENT = 1\n",
    "OPPONENT = -1\n",
    "NO_PLAYER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:    \n",
    "    def __init__(self, game_state=None):\n",
    "        if game_state is None:\n",
    "            game_state = [\n",
    "                0, 0, 0,\n",
    "                0, 0, 0,\n",
    "                0, 0, 0\n",
    "            ]\n",
    "        self.state = game_state\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.state)\n",
    "\n",
    "    def is_draw(self):\n",
    "        return len([field for field in self.state if field == NO_PLAYER]) == 0\n",
    "\n",
    "    def is_finished(self):\n",
    "        return self.get_winner() != NO_PLAYER or self.is_draw()\n",
    "\n",
    "    def valid_moves(self):\n",
    "        return [i for i in range(9) if self.state[i] == NO_PLAYER]\n",
    "\n",
    "    def make_move(self, field, player):\n",
    "        next = list(self.state)\n",
    "        next[field] = player\n",
    "        return Game(next)\n",
    "\n",
    "    def get_winner(self):\n",
    "        state = self.state\n",
    "        for i in range(3):\n",
    "            if state[i * 3] == state[i * 3 + 1] == state[i * 3 + 2] == state[i * 3] != NO_PLAYER:\n",
    "                return state[i * 3]\n",
    "            if state[i] == state[i + 3] == state[i + 6] == state[i] != NO_PLAYER:\n",
    "                return state[i]\n",
    "            if state[0] == state[4] == state[8] == state[0] != NO_PLAYER:\n",
    "                return state[0]\n",
    "            if state[2] == state[4] == state[6] == state[2] != NO_PLAYER:\n",
    "                return state[2]\n",
    "\n",
    "        return NO_PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(policy, opponent_policy, num_games=100):\n",
    "    games_won = 0\n",
    "    draw = 0\n",
    "    # Play games\n",
    "    for i in range(num_games):\n",
    "        game = Game()\n",
    "        # 50% chance opponent starts\n",
    "        if random.random() > 0.5:\n",
    "            game = game.make_move(opponent_policy(game), OPPONENT)\n",
    "\n",
    "        while not game.is_finished():\n",
    "            # First players turn\n",
    "            game = game.make_move(policy(game), AGENT)\n",
    "            if game.is_finished():\n",
    "                break\n",
    "            # Other players turn\n",
    "            game = game.make_move(opponent_policy(game), OPPONENT)\n",
    "\n",
    "        if game.get_winner() == 0:\n",
    "            draw = draw + 1\n",
    "        if game.get_winner() > 0:\n",
    "            games_won = games_won + 1\n",
    "\n",
    "    return games_won, draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(game):\n",
    "    return max(game.get_winner(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValuePolicy:\n",
    "    DEFAULT_VALUE = 0.5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.values = {}\n",
    "\n",
    "    def policy(self, game):\n",
    "        move_values = {}\n",
    "        moves = game.valid_moves()\n",
    "        for move in moves:\n",
    "            next = game.make_move(move, AGENT)\n",
    "            move_values[move] = self.get_state_value(next)\n",
    "\n",
    "        return max(move_values, key=move_values.get)\n",
    "\n",
    "    def get_state_value(self, state):\n",
    "        if str(state) not in self.values:\n",
    "            return self.DEFAULT_VALUE\n",
    "\n",
    "        return self.values[str(state)]\n",
    "\n",
    "    def set_state_value(self, state, value):\n",
    "        self.values[str(state)] = value\n",
    "\n",
    "    def learn(self, states):\n",
    "        # Actually perform the learning\n",
    "        def temporal_difference(current_state_value, next_state_value):\n",
    "            learning_rate = 0.1\n",
    "            return current_state_value + learning_rate * (next_state_value - current_state_value)\n",
    "\n",
    "        last_state = states[-1:][0]\n",
    "        last_value = reward(last_state)\n",
    "        self.set_state_value(last_state, last_value)\n",
    "        # Got through every state from end to start\n",
    "        for state in reversed(states[:-1]):\n",
    "            value = self.get_state_value(state)\n",
    "            last_value = temporal_difference(value, last_value)\n",
    "            self.set_state_value(state, last_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(game):\n",
    "    return random.choice(game.valid_moves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, opponent_policy, training_games=1000):\n",
    "    for i in range(training_games):\n",
    "        game = Game()\n",
    "        states = []\n",
    "\n",
    "        # 50% chance opponent starts\n",
    "        if random.random() > 0.5:\n",
    "            game = game.make_move(opponent_policy(game), OPPONENT)\n",
    "\n",
    "        while not game.is_finished():\n",
    "            # Our agent makes a move\n",
    "            # but occasionally we make a random choice\n",
    "            if random.random() < 0.5:\n",
    "                game = game.make_move(random_policy(game), AGENT)\n",
    "            else:\n",
    "                game = game.make_move(policy.policy(game), AGENT)\n",
    "            states.append(game)\n",
    "\n",
    "            if game.is_finished():\n",
    "                break\n",
    "\n",
    "            game = game.make_move(opponent_policy(game), OPPONENT)\n",
    "            states.append(game)\n",
    "\n",
    "        policy.learn(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games played: 1000\n",
      "Games won: 744\n",
      "Draw: 61\n"
     ]
    }
   ],
   "source": [
    "policy = ValuePolicy()\n",
    "\n",
    "train(policy, random_policy, training_games=1000)\n",
    "\n",
    "games_to_play = 1000\n",
    "games_won, draw = play_games(policy.policy, random_policy, games_to_play)\n",
    "\n",
    "print(\"Games played: %s\" % games_to_play)\n",
    "print(\"Games won: %s\" % games_won)\n",
    "print(\"Draw: %s\" % draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
