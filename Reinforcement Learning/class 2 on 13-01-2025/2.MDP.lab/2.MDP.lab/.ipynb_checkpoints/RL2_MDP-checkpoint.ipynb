{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Lab 2\n",
    "### J. Martinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Optional: finish the implementation of TicTacToe\n",
    "\n",
    "Remember that the general definition of the TD update rule is:\n",
    "\n",
    "$$ V(s_t) \\leftarrow V(s_t) + \\alpha[ V(s_{t+1}) - V(s_{t}] $$\n",
    "\n",
    "(We will come back to this later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Excercices\n",
    "\n",
    "### A) Examples\n",
    "Devise three example tasks of your own that fit into the reinforcement learning framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible.\n",
    "The framework is abstract and exible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Agent-environment boundary\n",
    "Consider the problem of driving. You could define the actions\n",
    "in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "\n",
    "Or you could define them farther out -- say, where the rubber meets the road, considering your actions to be tire torques.\n",
    "\n",
    "Or you could define them farther in -- say, where your brain meets your body, the actions being muscle twitches to control your limbs.\n",
    "\n",
    "Or you could go to a really high level and say that your actions are your choices of where to drive.\n",
    "\n",
    "What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Lazy robot\n",
    "Imagine that you are designing a robot to run a maze. You decide\n",
    "to give it a reward of +1 for escaping from the maze and a reward of zero at all other times.\n",
    "\n",
    "The task seems to break down naturally into episodes (the\n",
    "successive runs through the maze) so you decide to treat it as an episodic task, where the goal is to maximize expected total reward.\n",
    "\n",
    "After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### D Gridworld (from Sutton and Barto)\n",
    "\n",
    "The figure below shows a rectangular gridworld representation of a simple finite MDP.\n",
    "\n",
    "The cells of the grid correspond to the states of the environment.\n",
    "\n",
    "At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid.\n",
    "\n",
    "Actions that would take the agent oâ†µ the grid leave its location unchanged, but also result in a reward of -1.\n",
    "\n",
    "Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'.\n",
    "\n",
    "![Grid world](gridworld.png)\n",
    "\n",
    "Suppose the agent selects all four actions with equal probability in all states. \n",
    "\n",
    "The right part of the figure shows the value function, $v_\\pi$, for this policy, for the discounted reward case with $\\gamma$ = 0.9. This value function was computed by solving the system of linear equations (Bellman equation).\n",
    "\n",
    "Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy. State A is the best state to be in under this policy, but its expected return is less than 10, its immediate reward, because from A the agent is taken to A', from which it is likely to run into the edge of the grid. State B, on the other hand, is valued more than 5, its immediate reward, because from B the agent is taken to B', which has a positive value. From B' the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto A or B.\n",
    "\n",
    "The Bellman equation must hold for each state for the value function $v_\\pi$ shown in the figure (right). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
