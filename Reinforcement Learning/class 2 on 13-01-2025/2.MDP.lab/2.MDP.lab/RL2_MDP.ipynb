{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Lab 2\n",
    "### J. Martinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Optional: finish the implementation of TicTacToe\n",
    "\n",
    "Remember that the general definition of the TD update rule is:\n",
    "\n",
    "$$ V(s_t) \\leftarrow V(s_t) + \\alpha[ V(s_{t+1}) - V(s_{t}] $$\n",
    "\n",
    "(We will come back to this later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Excercices\n",
    "\n",
    "### A) Examples\n",
    "Devise three example tasks of your own that fit into the reinforcement learning framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible.\n",
    "The framework is abstract and exible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three diverse examples of tasks that fit into the reinforcement learning (RL) framework:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Autonomous Drone Delivery System**  \n",
    "**Domain**: Logistics and robotics.  \n",
    "\n",
    "- **States**:  \n",
    "  The drone's current position, battery level, wind conditions, and the location of packages and delivery points (e.g., a tuple: `(x, y, battery, wind)`).\n",
    "\n",
    "- **Actions**:  \n",
    "  Move up, down, left, right, hover, or return to base for recharging.\n",
    "\n",
    "- **Rewards**:  \n",
    "  - +10 for successfully delivering a package.  \n",
    "  - -5 for running out of battery mid-air.  \n",
    "  - -1 for unnecessary movement (to encourage efficiency).  \n",
    "  - +2 for charging when battery is low but not empty.\n",
    "\n",
    "This example tests the RL framework with continuous and dynamic environments involving physical constraints and uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Personalized Learning System**  \n",
    "**Domain**: Education technology.  \n",
    "\n",
    "- **States**:  \n",
    "  The student’s current knowledge level in different topics, past quiz performance, and time spent studying. For instance, a vector `[0.7, 0.5, 0.9]` might represent proficiency levels in three topics.\n",
    "\n",
    "- **Actions**:  \n",
    "  Present a new question, suggest a revision video, give a quiz, or recommend taking a break.\n",
    "\n",
    "- **Rewards**:  \n",
    "  - +5 for improved quiz performance compared to a baseline.  \n",
    "  - -2 for disengagement (e.g., skipping content).  \n",
    "  - +3 for completing tasks successfully within a time frame.  \n",
    "\n",
    "This example uses RL to adapt to human behavior, demonstrating how RL can work in personalized, user-centric domains.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Alien Ecosystem Explorer**  \n",
    "**Domain**: Speculative science fiction/environmental exploration.  \n",
    "\n",
    "- **States**:  \n",
    "  The current biome type (e.g., forest, desert, water), resource levels (e.g., oxygen, food, energy), and discovered alien species (e.g., a tuple: `(biome, oxygen, species_found)`).\n",
    "\n",
    "- **Actions**:  \n",
    "  Explore the current biome, move to a neighboring biome, collect resources, or attempt to interact with alien species.\n",
    "\n",
    "- **Rewards**:  \n",
    "  - +10 for discovering a new alien species.  \n",
    "  - -10 for running out of oxygen.  \n",
    "  - +5 for finding rare resources.  \n",
    "  - -1 for revisiting a biome with no new discoveries.  \n",
    "\n",
    "This task pushes the RL framework into speculative and exploratory domains, involving imaginative challenges and unknown dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights:\n",
    "1. **Autonomous Drone**: RL is applied to solve a real-world logistics problem with physical and dynamic constraints.\n",
    "2. **Personalized Learning**: RL adapts to human-centric tasks requiring nuanced understanding of behavioral patterns.\n",
    "3. **Alien Ecosystem Explorer**: RL stretches its application into a speculative domain, emphasizing exploration and discovery in unknown environments.\n",
    "\n",
    "These examples highlight RL's broad applicability across different domains, from grounded real-world problems to imaginative scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Agent-environment boundary\n",
    "Consider the problem of driving. You could define the actions\n",
    "in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "\n",
    "Or you could define them farther out -- say, where the rubber meets the road, considering your actions to be tire torques.\n",
    "\n",
    "Or you could define them farther in -- say, where your brain meets your body, the actions being muscle twitches to control your limbs.\n",
    "\n",
    "Or you could go to a really high level and say that your actions are your choices of where to drive.\n",
    "\n",
    "What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Lazy robot\n",
    "Imagine that you are designing a robot to run a maze. You decide\n",
    "to give it a reward of +1 for escaping from the maze and a reward of zero at all other times.\n",
    "\n",
    "The task seems to break down naturally into episodes (the\n",
    "successive runs through the maze) so you decide to treat it as an episodic task, where the goal is to maximize expected total reward.\n",
    "\n",
    "After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### D Gridworld (from Sutton and Barto)\n",
    "\n",
    "The figure below shows a rectangular gridworld representation of a simple finite MDP.\n",
    "\n",
    "The cells of the grid correspond to the states of the environment.\n",
    "\n",
    "At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid.\n",
    "\n",
    "Actions that would take the agent o↵ the grid leave its location unchanged, but also result in a reward of -1.\n",
    "\n",
    "Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'.\n",
    "\n",
    "![Grid world](gridworld.png)\n",
    "\n",
    "Suppose the agent selects all four actions with equal probability in all states. \n",
    "\n",
    "The right part of the figure shows the value function, $v_\\pi$, for this policy, for the discounted reward case with $\\gamma$ = 0.9. This value function was computed by solving the system of linear equations (Bellman equation).\n",
    "\n",
    "Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy. State A is the best state to be in under this policy, but its expected return is less than 10, its immediate reward, because from A the agent is taken to A', from which it is likely to run into the edge of the grid. State B, on the other hand, is valued more than 5, its immediate reward, because from B the agent is taken to B', which has a positive value. From B' the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto A or B.\n",
    "\n",
    "The Bellman equation must hold for each state for the value function $v_\\pi$ shown in the figure (right). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here here:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
