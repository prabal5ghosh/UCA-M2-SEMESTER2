{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4967d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Q-table:\n",
      "[[ 0.          0.        ]\n",
      " [-0.101       0.6504407 ]\n",
      " [ 0.46092552  0.7829    ]\n",
      " [ 0.67441166  0.881     ]\n",
      " [ 0.73130448  0.99      ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Optimal Policy:\n",
      "[-1, 1, 1, 1, 1, -1]\n",
      "\n",
      "Visualized Policy:\n",
      "['Trap', '→', '→', '→', '→', 'Goal']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the environment\n",
    "num_states = 6  # Number of states in the 1D grid\n",
    "trap_state = 0  # Trap position\n",
    "goal_state = 5  # Goal position\n",
    "start_state = 2  # Starting position\n",
    "\n",
    "actions = [-1, 1]  # Possible actions: left (-1), right (+1)\n",
    "\n",
    "# Rewards\n",
    "rewards = np.zeros(num_states)\n",
    "rewards[trap_state] = -1  # Negative reward for the trap\n",
    "rewards[goal_state] = 1   # Positive reward for the goal\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((num_states, len(actions)))\n",
    "\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(actions))  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit: best action\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = start_state\n",
    "    while state != trap_state and state != goal_state:\n",
    "        action_idx = epsilon_greedy(state, epsilon)\n",
    "        action = actions[action_idx]\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        next_state = max(0, min(num_states - 1, state + action))\n",
    "        reward = rewards[next_state] - 0.01  # Include move cost\n",
    "\n",
    "        # Update Q-value\n",
    "        best_next_action = np.max(Q[next_state])\n",
    "        Q[state, action_idx] += alpha * (reward + gamma * best_next_action - Q[state, action_idx])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Derive policy from Q-table\n",
    "policy = np.argmax(Q, axis=1)\n",
    "policy_actions = [actions[a] for a in policy]\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal Q-table:\")\n",
    "print(Q)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(policy_actions)\n",
    "\n",
    "\n",
    "def visualize_policy(policy):\n",
    "    grid = [' '] * num_states  # Initialize the grid with empty spaces\n",
    "    grid[trap_state] = 'Trap'\n",
    "    grid[goal_state] = 'Goal'\n",
    "    arrows = ['←' if action == -1 else '→' for action in policy]\n",
    "    for i, arrow in enumerate(arrows):\n",
    "        if i == trap_state or i == goal_state:\n",
    "            continue\n",
    "        grid[i] = arrow\n",
    "    return grid\n",
    "\n",
    "\n",
    "visualized_policy = visualize_policy(policy)\n",
    "print(\"\\nVisualized Policy:\")\n",
    "print(visualized_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbf03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35624654",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m action \u001b[38;5;241m=\u001b[39m actions[action_idx]\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Take action and observe next state and reward\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(num_states \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, state \u001b[38;5;241m+\u001b[39m action))\n\u001b[0;32m     64\u001b[0m reward \u001b[38;5;241m=\u001b[39m rewards[next_state] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Include move cost\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Update Q-value\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the environment\n",
    "rows, cols = 3, 4  # Grid size (3x4)\n",
    "trap_state = (1, 3)  # Trap position\n",
    "goal_state = (0, 3)  # Goal position\n",
    "start_state = (2, 0)  # Starting position\n",
    "\n",
    "# Possible actions\n",
    "actions = {\"UP\": (-1, 0), \"DOWN\": (1, 0), \"LEFT\": (0, -1), \"RIGHT\": (0, 1)}\n",
    "\n",
    "# Rewards\n",
    "rewards = np.full((rows, cols), -0.01)  # Default move cost\n",
    "rewards[trap_state] = -1  # Negative reward for the trap\n",
    "rewards[goal_state] = 1   # Positive reward for the goal\n",
    "\n",
    "# Stochastic environment probabilities\n",
    "prob_success = 0.8\n",
    "prob_lateral = 0.1\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((rows, cols, len(actions)))\n",
    "\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(list(actions.keys()))  # Explore: random action\n",
    "    else:\n",
    "        return list(actions.keys())[np.argmax(Q[state[0], state[1]])]  # Exploit: best action\n",
    "\n",
    "def step(state, action):\n",
    "    # Determine next state based on chosen action and stochastic outcomes\n",
    "    next_state = tuple(np.add(state, actions[action]))\n",
    "\n",
    "    # Ensure the agent remains within grid bounds\n",
    "    if next_state[0] < 0 or next_state[0] >= rows or next_state[1] < 0 or next_state[1] >= cols:\n",
    "        next_state = state  # Bounce back\n",
    "\n",
    "    # Apply stochasticity\n",
    "    random_value = np.random.rand()\n",
    "    if random_value < prob_success:\n",
    "        return next_state\n",
    "    elif random_value < prob_success + prob_lateral:\n",
    "        lateral_action = [a for a in actions if a != action and actions[a][0] == actions[action][1]]\n",
    "        return tuple(np.add(state, actions[np.random.choice(lateral_action)]))\n",
    "    else:\n",
    "        return state\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_enepisodes)]:\n",
    "\n",
    "    \n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = start_state\n",
    "    while state != trap_state and state != goal_state:\n",
    "        action_idx = epsilon_greedy(state, epsilon)\n",
    "        action = actions[action_idx]\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        next_state = max(0, min(num_states - 1, state + action))\n",
    "        reward = rewards[next_state] - 0.01  # Include move cost\n",
    "\n",
    "        # Update Q-value\n",
    "        best_next_action = np.max(Q[next_state])\n",
    "        Q[state, action_idx] += alpha * (reward + gamma * best_next_action - Q[state, action_idx])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Derive policy from Q-table\n",
    "policy = np.argmax(Q, axis=1)\n",
    "policy_actions = [actions[a] for a in policy]\n",
    "\n",
    "# Display results\n",
    "print(\"Optimal Q-table:\")\n",
    "print(Q)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(policy_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52a2592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Q-table:\n",
      "[[[ 0.074004    0.74293225 -0.0029701   0.05772543]\n",
      "  [ 0.5329027   0.89        0.75560792  0.64635922]\n",
      "  [ 0.77022513  1.          0.8644691   0.63218413]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.13016274  0.69928865  0.07728472  0.09917792]\n",
      "  [ 0.54140587  0.60936283  0.791       0.57143315]\n",
      "  [ 0.06830553 -0.19        0.87704753  0.07444601]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.50182575  0.62171     0.55803257  0.50210131]\n",
      "  [ 0.52719389  0.27432408  0.7019      0.50723312]\n",
      "  [ 0.52726791 -0.00444341  0.04263039 -0.0029701 ]\n",
      "  [-0.0039178  -0.0039404  -0.1        -0.0039404 ]]]\n",
      "\n",
      "Visualized Policy:\n",
      "[['→' '→' '→' 'G']\n",
      " ['→' '↑' '↑' 'T']\n",
      " ['→' '↑' '←' '←']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "grid_size = (3, 4)  # 3x4 grid\n",
    "trap_position = (1, 3)  # Position of the trap\n",
    "goal_position = (0, 3)  # Position of the goal\n",
    "start_position = (2, 0)  # Starting position\n",
    "\n",
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # LEFT, RIGHT, UP, DOWN\n",
    "\n",
    "# Rewards\n",
    "rewards = np.full(grid_size, -0.01)  # Default move cost\n",
    "rewards[trap_position] = -1  # Negative reward for the trap\n",
    "rewards[goal_position] = 1   # Positive reward for the goal\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of episodes\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((*grid_size, len(actions)))\n",
    "\n",
    "\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(actions))  # Explore: random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit: best action\n",
    "\n",
    "\n",
    "def is_terminal(state):\n",
    "    return state == trap_position or state == goal_position\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = start_position\n",
    "    while not is_terminal(state):\n",
    "        action_idx = epsilon_greedy(state, epsilon)\n",
    "        action = actions[action_idx]\n",
    "\n",
    "        # Calculate next state\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "        # Check for boundaries (bouncing walls)\n",
    "        next_state = (\n",
    "            max(0, min(grid_size[0] - 1, next_state[0])),\n",
    "            max(0, min(grid_size[1] - 1, next_state[1]))\n",
    "        )\n",
    "\n",
    "        # Get reward\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        # Update Q-value\n",
    "        best_next_action = np.max(Q[next_state])\n",
    "        Q[state][action_idx] += alpha * (reward + gamma * best_next_action - Q[state][action_idx])\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "# Derive policy from Q-table\n",
    "policy = np.argmax(Q, axis=2)\n",
    "\n",
    "# Visualize results\n",
    "arrows = {0: '←', 1: '→', 2: '↑', 3: '↓'}\n",
    "\n",
    "def visualize_policy(policy):\n",
    "    grid = np.full(grid_size, ' ')\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            if (i, j) == trap_position:\n",
    "                grid[i, j] = 'Trap'\n",
    "            elif (i, j) == goal_position:\n",
    "                grid[i, j] = 'Goal'\n",
    "            else:\n",
    "                grid[i, j] = arrows[policy[i, j]]\n",
    "    return grid\n",
    "\n",
    "\n",
    "print(\"Optimal Q-table:\")\n",
    "print(Q)\n",
    "print(\"\\nVisualized Policy:\")\n",
    "visualized_policy = visualize_policy(policy)\n",
    "print(visualized_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54946a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
