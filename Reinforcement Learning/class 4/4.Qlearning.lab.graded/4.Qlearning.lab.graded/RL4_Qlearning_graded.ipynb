{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Lab 4 (graded)\n",
    "### J. Martinet\n",
    "\n",
    "Implement Q-learning from scratch\n",
    "\n",
    "Duration: 90 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) First version with a 1D grid world\n",
    "\n",
    "\n",
    "We have discussed Q-learning during the class. As you know, it is an off-policy algorithm that uses the Time Difference $\\delta_t$, which is the difference between the estimated value of $s_t$ and the better estimate $r_{t+1} + \\gamma V^\\pi (s_{t+1})$\n",
    "\n",
    "$$ \\delta_t = r_{t+1} + \\gamma V^\\pi (s_{t+1}) - V^\\pi (s_t) $$\n",
    "\n",
    "The general definition of Q-learning update rule is:\n",
    "\n",
    "$$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha[ r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t,a_t) ] $$\n",
    "\n",
    "\n",
    "In this part, we are going to implement Q-learning in the simple setting of a 1D grid world:\n",
    "\n",
    "![1D grid world](RL4_1dgrid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand:\n",
    "- the size of the grid world (= number of states)\n",
    "- the size of the action space (= number of possible actions)\n",
    "- the size of the Q-table\n",
    "- the expected reward for reaching each state\n",
    "\n",
    "The first step will be to initialize an empty Q-table, a table of rewards, a move cost, and alpha and gamma parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we have 2 actions : move left and move right\n",
    "nb_action = 2\n",
    "nb_state = 6\n",
    "\n",
    "# we create a matrix 6*2 to represent the value of each move at a given state\n",
    "QTable = np.zeros((nb_state,nb_action))\n",
    "\n",
    "# the tab with the representation of the 6 states (-1 for the bad end, 1 for the good end, and 0 for other states)\n",
    "reward = [-1,0,0,0,0,1 ]\n",
    "\n",
    "# cost of one move\n",
    "cost = 0.01\n",
    "\n",
    "# learning rate - should not be too high, e.g. between .5 and .9\n",
    "alpha = 0.9\n",
    "\n",
    "# discount factor that shows how much you care about future (remember 0 for myopic)\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the interesting part. You need to write the main Q-learning loop.\n",
    "\n",
    "The first version will simply iterate:\n",
    "- choose an action (by looking up in the Q-table! Choose the most interesting move)\n",
    "- move\n",
    "- update the Q-table\n",
    "\n",
    "When you get this version, you can make it more complete to add the exploration/exploitation with the $\\epsilon$-greedy version, by initializing an $\\epsilon = 1$ that you decrease by e.g. 0.01 in each iteration.\n",
    "\n",
    "In your main loop, start by drawing a random number. If it is lower that $\\epsilon$, then EXPLORE (= take a random move), otherwise EXPLOIT (= choose the best move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Second version with a 2D grid world\n",
    "\n",
    "Same exercise, in the following 2D grid:\n",
    "\n",
    "![2D grid world](RL4_2dgrid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Optional third part (with bonus): plot the evolution of the total reward\n",
    "\n",
    "Make a plot of the evolution of the total reward after each epidode during the simulation / learning with different values of $\\gamma$, $\\alpha$, and $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
