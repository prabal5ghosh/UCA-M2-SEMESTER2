{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prabal Ghosh\n",
        "\n",
        "**RL LAB**"
      ],
      "metadata": {
        "id": "xI-dGzoPLIUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "aa8eBTuiL818"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Consider a 1D grid with :\n",
        "\n",
        " one goal location (positive reward, e.g. +1)\n",
        "\n",
        " one trap location (negative reward, e.g.-1)\n",
        "\n",
        " a fixed move cost (e.g.-0.01)\n",
        "\n",
        " deterministic actions (probability to go left when trying left is 1)"
      ],
      "metadata": {
        "id": "Eql9HverLH7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the environment"
      ],
      "metadata": {
        "id": "hw2nkOyiB6KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here our environment is 1 dimensional"
      ],
      "metadata": {
        "id": "zfFewHhQCDWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_states = 6  # Number of states\n",
        "trap_state = 0\n",
        "goal_state = 5\n",
        "start_state = 2  # Starting position\n",
        "move_cost = 0.01   # cost of moving one step in any direction"
      ],
      "metadata": {
        "id": "frS0UyeCBwHX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = [-1, 1]  # Possible actions: left (-1), right (+1)"
      ],
      "metadata": {
        "id": "1LA_f7JwByPl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rewards\n",
        "rewards = np.zeros(num_states)\n",
        "rewards[trap_state] = -1\n",
        "rewards[goal_state] = 1"
      ],
      "metadata": {
        "id": "Of2MRz_9CMjc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters\n"
      ],
      "metadata": {
        "id": "dKasd6fJCwRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon = 0.1  # Exploration rate\n",
        "num_episodes = 500"
      ],
      "metadata": {
        "id": "SZ0PdRgkCMf9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-table"
      ],
      "metadata": {
        "id": "m5VKNe3jC_H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-table\n",
        "Q = np.zeros((num_states, len(actions)))\n",
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7l6MRSuCMcn",
        "outputId": "3dc72c75-270b-4345-e9cc-ff3152738799"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### epsilon greedy action selection"
      ],
      "metadata": {
        "id": "niufJMvPDSEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(actions))  # choose any random action\n",
        "    else:\n",
        "        return np.argmax(Q[state])  # choose the best action"
      ],
      "metadata": {
        "id": "qEea4RfHDFGh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.choice(len(actions))"
      ],
      "metadata": {
        "id": "xtik8h-oD4lv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q[3] = [3,1]\n",
        "# Q"
      ],
      "metadata": {
        "id": "HxpAX-ZvEVum"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argmax(Q[3])"
      ],
      "metadata": {
        "id": "ocLUkaDfENtE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning from the equation, with-greedy"
      ],
      "metadata": {
        "id": "HcGWZQFpLnw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "    state = start_state\n",
        "    while state != trap_state and state != goal_state:\n",
        "        action_idx = epsilon_greedy(state, epsilon)\n",
        "        action = actions[action_idx]\n",
        "\n",
        "        next_state = max(0, min(num_states - 1, state + action))\n",
        "        reward = rewards[next_state] - move_cost\n",
        "\n",
        "        # Update Q-value  (Q-learning from the equation, with-greedy)\n",
        "        best_next_action = np.max(Q[next_state])\n",
        "        Q[state, action_idx] += alpha * (reward + gamma * best_next_action - Q[state, action_idx])\n",
        "\n",
        "        state = next_state  # Move to next state"
      ],
      "metadata": {
        "id": "_6fS2gyFD-2y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = np.argmax(Q, axis=1)\n",
        "policy_actions = [actions[a] for a in policy]"
      ],
      "metadata": {
        "id": "psvMv1U8Gmd3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal Q-table:\")\n",
        "print(Q)\n",
        "print(\"\\n\")\n",
        "print(\"\\nOptimal Policy:\")\n",
        "print(policy_actions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Szu8kihG7z0",
        "outputId": "4157bd00-a8f5-4513-e46f-976bd7d3e15e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Q-table:\n",
            "[[ 0.          0.        ]\n",
            " [-0.27371     0.63852427]\n",
            " [ 0.4381714   0.7829    ]\n",
            " [ 0.65815932  0.881     ]\n",
            " [ 0.70383523  0.99      ]\n",
            " [ 0.          0.        ]]\n",
            "\n",
            "\n",
            "\n",
            "Optimal Policy:\n",
            "[-1, 1, 1, 1, 1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"policy\", policy)"
      ],
      "metadata": {
        "id": "iu8d97_7JBIi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 2. Extend to the 2D grid of the classical toy example (lecture 1)"
      ],
      "metadata": {
        "id": "YJcmOCJPLwnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the environment"
      ],
      "metadata": {
        "id": "i2xrn-zQMEoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here the environment is 2 D Grid"
      ],
      "metadata": {
        "id": "RRuz9YVgMHOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_size = (3, 4)\n",
        "trap_position = (1, 3)\n",
        "goal_position = (0, 3)\n",
        "start_position = (2, 0)\n",
        "move_cost = 0.01   # cost of moving one step in any direction\n"
      ],
      "metadata": {
        "id": "GH5KzZj9L2kL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # LEFT, RIGHT, UP, DOWN\n"
      ],
      "metadata": {
        "id": "8MM-LdDZL2gi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rewards\n",
        "rewards = np.full(grid_size, -move_cost)  # Default move cost\n",
        "rewards[trap_position] = -1\n",
        "rewards[goal_position] = 1"
      ],
      "metadata": {
        "id": "AQ5fwnwVL2ds"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69vlPXqWM_lo",
        "outputId": "510fe060-734f-4274-a276-f97a68c486fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01, -0.01, -0.01,  1.  ],\n",
              "       [-0.01, -0.01, -0.01, -1.  ],\n",
              "       [-0.01, -0.01, -0.01, -0.01]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters\n"
      ],
      "metadata": {
        "id": "QdlNvRRyNXR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters are same as the previous question"
      ],
      "metadata": {
        "id": "IAujtkCoNile"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon = 0.1  # Exploration rate\n",
        "num_episodes = 1000"
      ],
      "metadata": {
        "id": "6KVrW4DVL2Xp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-table"
      ],
      "metadata": {
        "id": "S_1_flJfNtjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-table\n",
        "Q = np.zeros((*grid_size, len(actions)))\n",
        "Q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5mpwYS6Npxo",
        "outputId": "44ceaa54-6040-4627-886c-4454d74177be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### epsilon greedy action selection"
      ],
      "metadata": {
        "id": "NtgNFAVmN9cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(actions))  # choose any random action\n",
        "    else:\n",
        "        return np.argmax(Q[state])  # choose the best action"
      ],
      "metadata": {
        "id": "L5HOWr_KN_Pq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_terminal(state):\n",
        "    return state == trap_position or state == goal_position"
      ],
      "metadata": {
        "id": "5ej_pU5xN_M-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning from the equation, with-greedy"
      ],
      "metadata": {
        "id": "QBaQHheCOnp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "    state = start_position\n",
        "    while not is_terminal(state):\n",
        "        action_idx = epsilon_greedy(state, epsilon)\n",
        "        action = actions[action_idx]\n",
        "\n",
        "        next_state = (state[0] + action[0], state[1] + action[1])\n",
        "\n",
        "        next_state = (\n",
        "            max(0, min(grid_size[0] - 1, next_state[0])),\n",
        "            max(0, min(grid_size[1] - 1, next_state[1]))\n",
        "        )\n",
        "\n",
        "        reward = rewards[next_state]   # get reward\n",
        "\n",
        "        # Update Q-value  (Q-learning from the equation, with-greedy)\n",
        "        best_next_action = np.max(Q[next_state])\n",
        "        Q[state][action_idx] += alpha * (reward + gamma * best_next_action - Q[state][action_idx])\n",
        "\n",
        "        state = next_state  # Move to next state\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vaguQscbJdbZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal Q-table:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18I0J7H-PVIL",
        "outputId": "fd572639-d1bc-48b3-fa57-2e6adbbaeca4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Q-table:\n",
            "[[[-0.00385219  0.7332382   0.10517699  0.10071765]\n",
            "  [ 0.51927848  0.89        0.75236785  0.66440605]\n",
            "  [ 0.75305216  1.          0.83249637  0.66934196]\n",
            "  [ 0.          0.          0.          0.        ]]\n",
            "\n",
            " [[-0.00580175  0.69794129  0.00172136  0.09987948]\n",
            "  [ 0.53487345  0.65921544  0.791       0.59173985]\n",
            "  [-0.00109    -0.1         0.88455361  0.03174898]\n",
            "  [ 0.          0.          0.          0.        ]]\n",
            "\n",
            " [[ 0.51335856  0.62171     0.52522491  0.47846276]\n",
            "  [ 0.50957724  0.4168052   0.7019      0.57982255]\n",
            "  [ 0.58232454 -0.00271     0.07582799 -0.00199   ]\n",
            "  [ 0.03492463 -0.001      -0.1        -0.001     ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy = np.argmax(Q, axis=2)\n",
        "\n",
        "policy_actions = [actions[a] for a in policy.flatten()]\n",
        "print(\"policy_actions\", policy_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OlT3xvIO9ZQ",
        "outputId": "327f3c98-80b9-46a7-bf26-f21bb73e0258"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_actions [(0, 1), (0, 1), (0, 1), (0, -1), (0, 1), (-1, 0), (-1, 0), (0, -1), (0, 1), (-1, 0), (0, -1), (0, -1)]\n"
          ]
        }
      ]
    }
  ]
}