{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI-dGzoPLIUM"
   },
   "source": [
    "# Prabal Ghosh\n",
    "\n",
    "**RL LAB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aa8eBTuiL818"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eql9HverLH7m"
   },
   "source": [
    "# 1. Consider a 1D grid with :\n",
    "\n",
    " one goal location (positive reward, e.g. +1)\n",
    "\n",
    " one trap location (negative reward, e.g.-1)\n",
    "\n",
    " a fixed move cost (e.g.-0.01)\n",
    "\n",
    " deterministic actions (probability to go left when trying left is 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw2nkOyiB6KS"
   },
   "source": [
    "### Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfFewHhQCDWD"
   },
   "source": [
    "Here our environment is 1 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "frS0UyeCBwHX"
   },
   "outputs": [],
   "source": [
    "num_states = 6  # Number of states\n",
    "trap_state = 0\n",
    "goal_state = 5\n",
    "start_state = 2  # Starting position\n",
    "move_cost = 0.01   # cost of moving one step in any direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1LA_f7JwByPl"
   },
   "outputs": [],
   "source": [
    "actions = [-1, 1]  # Possible actions: left (-1), right (+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Of2MRz_9CMjc"
   },
   "outputs": [],
   "source": [
    "# Rewards\n",
    "rewards = np.zeros(num_states)\n",
    "rewards[trap_state] = -1\n",
    "rewards[goal_state] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKasd6fJCwRs"
   },
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SZ0PdRgkCMf9"
   },
   "outputs": [],
   "source": [
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5VKNe3jC_H_"
   },
   "source": [
    "### Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7l6MRSuCMcn",
    "outputId": "3dc72c75-270b-4345-e9cc-ff3152738799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((num_states, len(actions)))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niufJMvPDSEu"
   },
   "source": [
    "### epsilon greedy action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qEea4RfHDFGh"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(actions))  # choose any random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # choose the best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xtik8h-oD4lv"
   },
   "outputs": [],
   "source": [
    "# np.random.choice(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HxpAX-ZvEVum"
   },
   "outputs": [],
   "source": [
    "# Q[3] = [3,1]\n",
    "# Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ocLUkaDfENtE"
   },
   "outputs": [],
   "source": [
    "# np.argmax(Q[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcGWZQFpLnw9"
   },
   "source": [
    "Q-learning from the equation, with-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_6fS2gyFD-2y"
   },
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = start_state\n",
    "    while state != trap_state and state != goal_state:\n",
    "        action_idx = epsilon_greedy(state, epsilon)\n",
    "        action = actions[action_idx]\n",
    "\n",
    "        next_state = max(0, min(num_states - 1, state + action))\n",
    "        reward = rewards[next_state] - move_cost\n",
    "\n",
    "        # Update Q-value  (Q-learning from the equation, with-greedy)\n",
    "        best_next_action = np.max(Q[next_state])\n",
    "        Q[state, action_idx] += alpha * (reward + gamma * best_next_action - Q[state, action_idx])\n",
    "\n",
    "        state = next_state  # Move to next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "psvMv1U8Gmd3"
   },
   "outputs": [],
   "source": [
    "policy = np.argmax(Q, axis=1)\n",
    "policy_actions = [actions[a] for a in policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Szu8kihG7z0",
    "outputId": "4157bd00-a8f5-4513-e46f-976bd7d3e15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Q-table:\n",
      "[[ 0.          0.        ]\n",
      " [-0.27371     0.63852427]\n",
      " [ 0.4381714   0.7829    ]\n",
      " [ 0.65815932  0.881     ]\n",
      " [ 0.70383523  0.99      ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Optimal Policy:\n",
      "[-1, 1, 1, 1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Q-table:\")\n",
    "print(Q)\n",
    "print(\"\\n\")\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(policy_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iu8d97_7JBIi"
   },
   "outputs": [],
   "source": [
    "# print(\"policy\", policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJcmOCJPLwnm"
   },
   "source": [
    " # 2. Extend to the 2D grid of the classical toy example (lecture 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2xrn-zQMEoA"
   },
   "source": [
    "### Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRuz9YVgMHOl"
   },
   "source": [
    "here the environment is 2 D Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GH5KzZj9L2kL"
   },
   "outputs": [],
   "source": [
    "grid_size = (3, 4)\n",
    "trap_position = (1, 3)\n",
    "goal_position = (0, 3)\n",
    "start_position = (2, 0)\n",
    "move_cost = 0.01   # cost of moving one step in any direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8MM-LdDZL2gi"
   },
   "outputs": [],
   "source": [
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # LEFT, RIGHT, UP, DOWN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AQ5fwnwVL2ds"
   },
   "outputs": [],
   "source": [
    "# Rewards\n",
    "rewards = np.full(grid_size, -move_cost)  # Default move cost\n",
    "rewards[trap_position] = -1\n",
    "rewards[goal_position] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69vlPXqWM_lo",
    "outputId": "510fe060-734f-4274-a276-f97a68c486fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01, -0.01, -0.01,  1.  ],\n",
       "       [-0.01, -0.01, -0.01, -1.  ],\n",
       "       [-0.01, -0.01, -0.01, -0.01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdlNvRRyNXR2"
   },
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAujtkCoNile"
   },
   "source": [
    "Hyperparameters are same as the previous question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6KVrW4DVL2Xp"
   },
   "outputs": [],
   "source": [
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_1_flJfNtjO"
   },
   "source": [
    "### Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5mpwYS6Npxo",
    "outputId": "44ceaa54-6040-4627-886c-4454d74177be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((*grid_size, len(actions)))\n",
    "Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtgNFAVmN9cw"
   },
   "source": [
    "### epsilon greedy action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "L5HOWr_KN_Pq"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(actions))  # choose any random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # choose the best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5ej_pU5xN_M-"
   },
   "outputs": [],
   "source": [
    "def is_terminal(state):\n",
    "    return state == trap_position or state == goal_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBaQHheCOnp7"
   },
   "source": [
    "Q-learning from the equation, with-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vaguQscbJdbZ"
   },
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = start_position\n",
    "    while not is_terminal(state):\n",
    "        action_idx = epsilon_greedy(state, epsilon)\n",
    "        action = actions[action_idx]\n",
    "\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "        next_state = (\n",
    "            max(0, min(grid_size[0] - 1, next_state[0])),\n",
    "            max(0, min(grid_size[1] - 1, next_state[1]))\n",
    "        )\n",
    "\n",
    "        reward = rewards[next_state]   # get reward\n",
    "\n",
    "        # Update Q-value  (Q-learning from the equation, with-greedy)\n",
    "        best_next_action = np.max(Q[next_state])\n",
    "        Q[state][action_idx] += alpha * (reward + gamma * best_next_action - Q[state][action_idx])\n",
    "\n",
    "        state = next_state  # Move to next state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18I0J7H-PVIL",
    "outputId": "fd572639-d1bc-48b3-fa57-2e6adbbaeca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Q-table:\n",
      "[[[-0.00385219  0.7332382   0.10517699  0.10071765]\n",
      "  [ 0.51927848  0.89        0.75236785  0.66440605]\n",
      "  [ 0.75305216  1.          0.83249637  0.66934196]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.00580175  0.69794129  0.00172136  0.09987948]\n",
      "  [ 0.53487345  0.65921544  0.791       0.59173985]\n",
      "  [-0.00109    -0.1         0.88455361  0.03174898]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.51335856  0.62171     0.52522491  0.47846276]\n",
      "  [ 0.50957724  0.4168052   0.7019      0.57982255]\n",
      "  [ 0.58232454 -0.00271     0.07582799 -0.00199   ]\n",
      "  [ 0.03492463 -0.001      -0.1        -0.001     ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Q-table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OlT3xvIO9ZQ",
    "outputId": "327f3c98-80b9-46a7-bf26-f21bb73e0258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_actions [(0, 1), (0, 1), (0, 1), (0, -1), (0, 1), (-1, 0), (-1, 0), (0, -1), (0, 1), (-1, 0), (0, -1), (0, -1)]\n"
     ]
    }
   ],
   "source": [
    "policy = np.argmax(Q, axis=2)\n",
    "\n",
    "policy_actions = [actions[a] for a in policy.flatten()]\n",
    "print(\"policy_actions\", policy_actions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
